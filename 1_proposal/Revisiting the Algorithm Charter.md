# Revisiting the Algorithm Charter
Johniel Bocacao
April 2024

## 1. Introduction
Set the scene, why I am doing this. Triangulate based on unique insight I have at ACC.
- Government agencies collect data on citizens that interact with it, use that data for algorithms to improve service delivery, target, predict etc. Talk about some algorithms that are out of scope to muddy the waters, e.g. CBAs.
- Government agencies need to have safeguards in place to ensure they use data for algorithms appropriately, ethically, with social license.
- Algorithm Charter developed to commit agencies to basic principles around transparency, accountability, and considerations during the algorithm lifecycle.
>  “demonstrate a commitment to ensuring New Zealanders have confidence in how government agencies use algorithms” through transparency and accountability in how they use New Zealanders’ data to make decisions about them.
- Agencies embed principles in their enterprise policies to truly take effect
- Year 1 review commissioned by StatsNZ in 2021 focused on operationalisation rather than a thorough, first principles review
### 1.1. Motivation
Problem statements
- Difficulties around actually being sufficiently transparent - different across agencies
- Are offline algorithms being monitored on an ongoing basis?
- Perennial question around how to best involve people/communities affected by the decisions automated
- Governments need clearer guidance on being a good Treaty partner in general - how to apply to the Charter. MASov?
- Always tricky to incorporate human oversight in the process, verify when these are soft controls
- Difficulty understanding what’s in scope
### 1.2. Research Objectives
Map to each motivation.
- Model register
- Recommendations for monitoring based on what agencies are required to report
- Recommendations for algorithm (co)design methodologies?
- Addressing quantification of bias. Rec #5: Important but difficult.
	- Definitions of fairness and unfairness?
	- Tradeoffs between different sorts…
- Recommendations on what’s in and out of scope?
## 2. Background
### 2.1. Algorithms in the Public Sector
Define what’s in scope of AC
Examples of algorithms, what’s not an algorithm
Generative AI - pending question
Interim Generative AI guidelines for NZ Government but only for services - not for GAI dev itself.
### 2.2. Algorithm Charter of Aotearoa New Zealand
- History and development, who did Stats interview
- Find suggestions from submissions made 
- Year 1 review
	- Interviews from agencies with high-risk algorithms
	- Interviews with civil society: academics, peak bodies, MDS/Tiriti experts
	- Questionnaires to analytics Tier 2s, non-signatory agencies.
	- Agencies were excluded on the basis of minimal use of algorithms.
	- Out of scope: amendments, private sector, **outcomes**.

### 2.3. Related Policies and Frameworks
#### Algorithm Impact Assessment
Independently drafted for assessing algorithm risk at the pre-deployment phase, including whether it is high risk enough to fall under the Charter.
#### Internal Agency Frameworks
PHRAE - MSD impact-side evaluation prior to deployment
MDL - MSD framework for model development and maintenance
DPUP - SWA
#### Ngā Tikanga Paihere
Stats framework for applying tikanga Māori to data use
#### Māori Algorithmic Sovereignty
As per Brown et al re *MAS: idea, principles use*, how they were transformed from MDS principles already circulated to agencies.
### 2.4. Related International Efforts
Canadian directives on responsible AI
Apply EU AI Act ideas in a non-commercial context
Biden administration EO, guidance to federal agencies on AI use
## 3. Timeline

| Phase    | 2   | 3   | 4    | 5   | 6   | 7   | 8   | 9    | 10  | 11  | 12  |
| -------- | --- | --- | ---- | --- | --- | --- | --- | ---- | --- | --- | --- |
| Analysis | Lit | X   | X    | Ana | X   |     |     |      |     |     |     |
| IV dev   |     | X   |      |     |     |     | X   |      |     |     |     |
| Pri res  |     |     | Init | X   |     |     |     | User | X   |     |     |
| Ideation |     |     |      | X   | X   |     |     |      | X   | X   |     |
| Solution |     |     |      |     | X   | X   | X   |      |     | X   | X   |

Will become a Gantt chart - 
### Critical analysis (10 months)
Critical analysis of the state of play across the breadth of the Charter?
Frame as not just a review but a contribution in its own right
#### Interview development (2 months)
Write up questions and script, devise sample
#### Primary research (4 months)
Undertake interviews
#### Ideation and Synthesis (overlapping 4 months)
Integrate learnings from research
### Solution development (overlapping 14 months)
Similar waterfall structure: develop tool, devise and perform user tests, integrate feedback, finalise solution
## Resources
### Interview Sample
Who is in and out of scope for interviewing, how I’ll build a representative sample
Ethical considerations
### Trial Data for Solution Development
What data I’ll be using to test solutions
