# Revisiting the Algorithm Charter

Johniel Bocacao

April 2024

## 1. Introduction

Set the scene, why I am doing this. Triangulate based on unique insight I have at ACC.

- Government agencies collect data on citizens that interact with it, use that data for algorithms to improve service delivery, target, predict etc. Talk about some algorithms that are out of scope to muddy the waters, e.g. CBAs.
- Government agencies need to have safeguards in place to ensure they use data for algorithms appropriately, ethically, with social license.
- Algorithm Charter developed to commit agencies to basic principles around transparency, accountability, and considerations during the algorithm lifecycle.
- Agencies embed principles in their enterprise policies to truly take effect
- Year 1 review commissioned by StatsNZ in 2021 focused on operationalisation rather than a thorough, first principles review

### 1.1. Motivation

Problem statements

- Difficulties around actually being sufficiently transparent - different across agencies
- Are offline algorithms being monitored on an ongoing basis?
- Perennial question around how to best involve people/communities affected by the decisions automated
- Governments need clearer guidance on being a good Treaty partner in general - how to apply to the Charter. MASov?
- Always tricky to incorporate human oversight in the process, verify when these are soft controls
- Difficulty understanding what’s in scope

### 1.2. Research Objectives

Map to each motivation.

- Model register
- Recommendations for monitoring based on what agencies are required to report
- Recommendations for algorithm (co)design methodologies?
- Addressing quantification of bias. Rec #5: Important but difficult.
    - Definitions of fairness and unfairness?
    - Tradeoffs between different sorts…
- Recommendations on what’s in and out of scope?

## 2. Background

### 2.1. Algorithms in the Public Sector

- Define what’s in scope of AC
- Examples of algorithms, what’s not an algorithm
- Generative AI - pending question
- Interim Generative AI guidelines for NZ Government but only for services - not for GAI dev itself.

### 2.2. Algorithm Charter of Aotearoa New Zealand

- History and development, who did Stats interview
- Find suggestions from submissions made 
- Year 1 review
    - Interviews from agencies with high-risk algorithms
    - Interviews with civil society: academics, peak bodies, MDS/Tiriti experts
    - Questionnaires to analytics Tier 2s, non-signatory agencies.
    - Agencies were excluded on the basis of minimal use of algorithms.
    - Out of scope: amendments, private sector, **outcomes**.
- Outstanding and actioned recommendations from the Year 1 review

### 2.3. Related Policies and Frameworks

#### 2.3.1. Algorithm Impact Assessment

Drafted as a result of Year 1 review for assessing algorithm risk at the pre-deployment phase, including whether it is high risk enough to fall under the Charter.

#### 2.3.2. Internal Agency Frameworks

- PHRAE - MSD impact-side evaluation prior to deployment
- MDL - MSD framework for model development and maintenance
- DPUP - SWA

#### 2.3.3. Ngā Tikanga Paihere

Stats framework for applying tikanga Māori to data use

#### 2.3.4. Māori Algorithmic Sovereignty

As per *(Brown et al., 2023)*, how they were transformed from MDS principles already circulated to agencies.

### 2.4. Related International Efforts

- Canadian directives on responsible AI
- Apply EU AI Act ideas in a non-commercial context
- Biden administration EO, guidance to federal agencies on AI use

## 3. Timeline

|          | 2  | 3 | 4    | 5   | 6 | 7 | 8 | 9    | 10 | 11 | 12 |
| -------- | -- | - | ---- | --- | - | - | - | ---- | -- | -- | -- |
| Analysis | LR | X | X    | Ana | X |   |   |      |    |    |    |
| Itvw dev |    | X |      |     |   |   | X |      |    |    |    |
| Pri res  |    |   | Disc | X   |   |   |   | User | X  |    |    |
| Idea     |    |   |      | X   | X |   |   |      | X  | X  |    |
| Sol      |    |   |      |     | X | X | X |      |    | X  | X  |
<p align = "center">Table 1 - Proposed Gantt chart (column represents 2 months)</p>


### 3.1. Critical Analysis (10 months)

Critical analysis of the state of play across the breadth of the Charter? Frame as not just a review but a contribution in its own right

#### 3.1.1. Interview development (2 months)

Write up questions and script, devise sample

#### 3.1.2. Primary research (4 months)

Undertake interviews

#### 3.1.3. Ideation and Synthesis (overlapping 4 months)

Integrate learnings from research

### 3.2. Solution development (overlapping 14 months)

Similar waterfall structure: develop tool, devise and perform user tests, integrate feedback, finalise solution

## 4. Resources

### 4.1. Interview Sample

Who is in and out of scope for interviewing, how I’ll build a representative sample. Ethical considerations.

### 4.2. Trial Data for Solution Development

What data I’ll be using to test solutions



